{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855060ed-6dc9-49c1-813e-13ec065b04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#RENAME \n",
    "\n",
    "dict = {\"name\":['sahil','rohit','keshav'],\n",
    "        \"id\":[101,102,103],\n",
    "        \"city\":['Pune','banglore','mumbai']}\n",
    "df = pd.DataFrame(dict)\n",
    "df.rename(columns={'name':'new_name'},index={1:'one'},inplace=True)\n",
    "indx_mapper = {}\n",
    "col_mapper = {}\n",
    "for col , indx in zip(df.columns,df.index):\n",
    "    indx_mapper[indx] = 'new_'+str(indx)\n",
    "    col_mapper[col] = 'new_'+str(col)\n",
    "#print(indx_mapper,col_mapper)\n",
    "df.rename(columns = col_mapper , index = indx_mapper , inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8bcaed-696f-4459-87a7-0c0ea2234721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROP\n",
    "\n",
    "df.drop(list(df.columns),axis=1)\n",
    "df.drop(list(df.index))\n",
    "\"\"\"\n",
    "for i , j in zip(df.columns,df.index):\n",
    "    inp = input()\n",
    "    if inp == 'y':\n",
    "        df.drop(columns=i,index=j,inplace=True)\n",
    "df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3312a-cb48-4141-bbe1-3e65bd68662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#DESCRIBE\n",
    "df = pd.DataFrame({\n",
    "    'A': [10, 20, 30, 40, 50],\n",
    "    'B': [5, 15, 25, 35, 45],\n",
    "    'C': ['x', 'y', 'x', 'y', 'x']\n",
    "})\n",
    "\n",
    "df.describe(include=[np.number])\n",
    "df.describe(include=['object'])\n",
    "df.describe(percentiles=[0.0009,0.023,0.885,0.88])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5994822-63b2-4178-9b26-4317e07e13b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = 100\n",
    "\n",
    "# Generate a huge DataFrame with 1 million rows and 200 columns\n",
    "data = {f'col_{i}': np.random.random(size=100) for i in range(num_columns)}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381395e-651d-4e9c-8ae3-71f02e203057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LOC , iLOC\n",
    "\n",
    "#df.loc[0:4,'A':'C'] # [a,b]\n",
    "#df.iloc[0:4,0:3] # [a,b)\n",
    "\n",
    "\n",
    "\n",
    "indx_list = []\n",
    "col_list = []\n",
    "for col , indx in zip(df.columns,df.index):\n",
    "    if indx & 1:\n",
    "        indx_list.append(indx)\n",
    "        col_list.append(col)\n",
    "\n",
    "df.drop(columns = col_list , index = indx_list , inplace = True)\n",
    "df_modified = df\n",
    "\n",
    "#df_modified = df.drop[columns = [col+f'_{int(col.split('_')[-1])-1}' for col in col_list] , index = [indx - 1 for indx in indx_list] , inplace = True]\n",
    "\n",
    "indx_mapper = {}\n",
    "col_mapper = {}\n",
    "for col , indx in zip(df_modified.columns,df_modified.index):\n",
    "    indx_mapper[indx] = 'new_'+str(indx)\n",
    "    col_mapper[col] = 'new_'+str(col)\n",
    "#print(indx_mapper,col_mapper)\n",
    "df_modified.rename(columns = col_mapper , index = indx_mapper , inplace = True)\n",
    "df_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40f6d0-9f15-4c85-b9ea-2d267b43afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data)\n",
    "df2\n",
    "\n",
    "df2.iloc[95:,0] = df2.iloc[25:30,78]\n",
    "df2['col_0'].min()\n",
    "\n",
    "df2[df2['col_0'] == df2['col_0'].min()]\n",
    "\n",
    "#TAIL AND HEAD\n",
    "df2.iloc[95:,0] = df2.iloc[25:30,78]\n",
    "import time , cProfile\n",
    "def sort_each_col(df):\n",
    "    for col in df.columns:\n",
    "        df[col].sort_values(ascending=False)\n",
    "    tailed = df.iloc[:,0].tail(5)\n",
    "    for cf in df.columns[1:]:\n",
    "        for low in df[cf].tail(5):\n",
    "            indx = 0\n",
    "            for lower in tailed:\n",
    "                if lower > low:\n",
    "                    tailed.iloc[indx] = low\n",
    "                    indx += 1\n",
    "    return tailed\n",
    "            \n",
    "start = time.time()        \n",
    "print(sort_each_col(df2))  \n",
    "end = time.time()\n",
    "print(end - start)\n",
    "#cProfile.run('sort_each_col(df2)')\n",
    "\n",
    "import time , cProfile\n",
    "df2.iloc[95:,0] = df2.iloc[25:30,78]\n",
    "def compare_each_col(df):\n",
    "    df.iloc[:,0] = df.iloc[:,0].sort_values(ascending=False)\n",
    "    tailed = df.iloc[:,0].tail(5)\n",
    "    for cf in df.columns[1:]:\n",
    "        for low in df[cf]:\n",
    "            indx = 0\n",
    "            for lower in tailed:\n",
    "                if lower > low:\n",
    "                    tailed.iloc[indx] = low\n",
    "                    indx += 1\n",
    "    return tailed\n",
    "start = time.time()        \n",
    "print(compare_each_col(df2))\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "#cProfile.run('compare_each_col(df2)')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def find_lowest_5(df):\n",
    "    # Flatten the DataFrame into a single Series, sort it, and return the lowest 5 values\n",
    "    return df.stack().sort_values().head(5).reset_index(drop=True)\n",
    "start = time.time()        \n",
    "print(find_lowest_5(df2))\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2456eaf5-71f1-49cb-a484-a926c3f72ad8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574c868-0e10-4fa5-8339-44e42f767950",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAMPLE\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'A': range(1, 11), 'B': range(11, 21)}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Get 3 random rows with a fixed random_state\n",
    "sample1 = df.sample(n=3, random_state=43)\n",
    "sample2 = df.sample(n=3, random_state=4)\n",
    "\n",
    "# These two will be identical\n",
    "print(sample1)\n",
    "print(sample2)\n",
    "\n",
    "sample3 = df.sample(frac=0.8)\n",
    "print(sample3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99a892-67fc-496a-86d2-1a44dcedc834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ITERROWS - (index , col(n) val(n)) val(n) - row[1][n]\n",
    "\n",
    "for row in df2.iterrows():\n",
    "    print(type(row[1]))\n",
    "    print(type(row[0]))\n",
    "    print(type(row[1][1]))\n",
    "    print(row[1].iloc[1])\n",
    "    print(row[1].keys())\n",
    "    print(row[1].values)\n",
    "    print(row[1].shape[0])\n",
    "    print(row[1].index[2])\n",
    "\n",
    "#ITERROWS - (index , col(n) val(n)) val(n) - row[1][n]\n",
    "\n",
    "label_dict = { str(row[0])+'-'+row[1].keys()[count] : row[1].iloc[count] for row in df2.iterrows() for count in range(0,df2.shape[0]) } \n",
    "label_dict['99-col_99']\n",
    "\n",
    "label_dict = {\n",
    "    str(row[0]) + '-' + row[1].index[count]: row[1].iloc[count] \n",
    "    for row in df2.iterrows() \n",
    "    for count in range(0, df2.shape[1])  # Loop over the number of columns, not rows\n",
    "}\n",
    "\n",
    "print(label_dict['99-col_99'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c17d32-9e65-4fa6-bdf9-633432b36b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DTYPES ASTYPES SELECTTYPES\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with an 'object' column\n",
    "data = {'Category': ['apple', 'banana', 'apple', 'cherry', 'banana', 'apple']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Check the data type of the 'Category' column\n",
    "print(df['Category'].dtype)  # Output: object\n",
    "\n",
    "# Convert the 'Category' column to 'category' type\n",
    "df['Category'] = df['Category'].astype('category')\n",
    "\n",
    "# Check the data type again\n",
    "print(df['Category'].dtype)  # Output: category\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a DataFrame with all types of columns\n",
    "data = {\n",
    "    'Integer': [1, 2, 3, 4, 5],\n",
    "    'Float': [1.1, 2.2, 3.3, 4.4, 5.5],\n",
    "    'Boolean': [True, False, True, False, True],\n",
    "    'String': ['apple', 'banana', 'cherry', 'date', 'elderberry'],\n",
    "    'Category': pd.Categorical(['small', 'medium', 'large', 'small', 'medium']),\n",
    "    'Datetime': pd.to_datetime(['2022-01-01', '2022-02-01', '2022-03-01', '2022-04-01', '2022-05-01']),\n",
    "    'Timedelta': pd.to_timedelta(['1 days', '2 days', '3 days', '4 days', '5 days']),\n",
    "    'Period': pd.period_range('2022-01', periods=5, freq='M'),\n",
    "    'Interval': pd.interval_range(start=1, end=11, freq=2) \n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display DataFrame\n",
    "display(df.dtypes)\n",
    "df.select_dtypes('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f415fb-b8aa-486e-970f-5d523f3aac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape[0]*df2.shape[1] == df2.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2dac5-5033-4c19-90c5-33ca0ed4d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDIM MEMORY_USAGE()\n",
    "\n",
    "df2.memory_usage(index=False , deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64db433-4553-4de0-bfb9-65e79fc69c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INDEX \n",
    "\n",
    "df2.index[13]\n",
    "len(df2.index)\n",
    "len(range(0,100))\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c94ec-d866-40e7-9899-3140371f71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PD.DATAFRAME()\n",
    "\n",
    "\n",
    "dictionary = {\"name\":['sahil','rohit','keshav','sahil','rohit','keshav','sahil','rohit','keshav'],\n",
    "        \"id\":[101,102,103,101,102,103,101,102,103],\n",
    "        \"city\":['Pune','banglore','mumbai','Pune','banglore','mumbai','Pune','banglore','mumbai']}\n",
    "\n",
    "df = pd.DataFrame(dictionary,index=pd.date_range('20250126',periods=9))#,columns=[1,2,3])\n",
    "print(df)\n",
    "df = pd.DataFrame(dictionary,index=pd.date_range('20250126',periods=9),columns=[1,2,3])\n",
    "print(df)\n",
    "\n",
    "\n",
    "data = [\n",
    "    ['Alice', 25, 'New York'],\n",
    "    ['Bob', 30, 'Los Angeles'],\n",
    "    ['Charlie', 35, 'Chicago']\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Name', 'Age', 'City'])\n",
    "print(df)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "df = pd.DataFrame(data, columns=['A', 'B', 'C'])\n",
    "print(df)\n",
    "\n",
    "\n",
    "data = [\n",
    "    {'Name': 'Alice', 'Age': 25, 'City': 'New York'},\n",
    "    {'Name': 'Bob', 'Age': 30, 'City': 'Los Angeles'},\n",
    "    {'Name': 'Charlie', 'Age': 35, 'City': 'Chicago'}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "print(df)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Original DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "\n",
    "# Assign df to df2 (both point to the same object)\n",
    "df2 = df\n",
    "\n",
    "# Modify df2\n",
    "df2.loc[0, 'A'] = 100\n",
    "\n",
    "# Print both DataFrames\n",
    "print(\"Original df:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nModified df2:\")\n",
    "print(df2)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4.5, 5.5, 6.5]\n",
    "}\n",
    "\n",
    "df_original = pd.DataFrame(data)  # Original DataFrame\n",
    "df_copy = pd.DataFrame(data, copy=True)  # Creates a new copy\n",
    "\n",
    "df_copy.loc[0, 'A'] = 100  # Modify copied DataFrame\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_original)\n",
    "\n",
    "print(\"\\nCopied DataFrame:\")\n",
    "print(df_copy)\n",
    "\n",
    "\n",
    "df.rename_axis('num',axis=0,inplace=True)\n",
    "df.set_index('A')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5898e8-47af-4035-b526-73e67ee76b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV\n",
    "\n",
    "#1st - url -- file path\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "df3 = pd.read_csv(url)\n",
    "df7 = pd.read_csv(url)\n",
    "df4 = pd.read_csv(url , sep = ';')\n",
    "df5 = pd.read_csv(url)\n",
    "#display(df5)\n",
    "#print(df5.loc[2,:].values)\n",
    "def swap_series(df, indx1, indx2):\n",
    "    df.loc[indx1, :], df.loc[indx2, :] = df.loc[indx2, :].copy(), df.loc[indx1, :].copy()\n",
    "def push_col(df, indx):\n",
    "    df.columns , df.loc[indx,:] = df.loc[indx,:] , df.columns\n",
    "push_col(df5,200)\n",
    "#display(df5)\n",
    "swap_series(df5,200,0)\n",
    "#display(df5)\n",
    "df5.to_csv('swapped.csv')\n",
    "df6 = pd.read_csv('swapped.csv')\n",
    "#display(df6)\n",
    "\n",
    "df7 = pd.read_csv(url , na_values = ['?','NaN','NULL','female'])\n",
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff7d2c0-a2c4-4e7b-a950-df2ef5125deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET INDEX , RESET INDEX , FILTER \n",
    "\n",
    "df3.set_index(['Survived','Name'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa2578-96a3-4baa-b045-8655c31b9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.set_index(['Ticket','Fare','Cabin','Survived','Name'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea043a9-e001-4162-8efa-529bca99c6c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SET INDEX , RESET INDEX , FILTER \n",
    "\n",
    "df3.filter(['Sex','Parch'],axis=1)\n",
    "df3.filter(like = 'James',axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4e556-01d5-45a7-9764-fb9bb95cf69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.reset_index()\n",
    "df3.set_index('Name',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b51f5-80d6-4901-a69e-4077eb5c12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df3)\n",
    "df3.filter(like = 'Cab' ) #by deafult axis = 1\n",
    "df3.filter(like = 'Jo' , axis =  0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f547e-f751-4401-b201-5a1f7027711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.set_index('Sex',drop=False,inplace=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7008bfc6-fdad-41f1-a92a-b46012fe53dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGG\n",
    "\n",
    "print(df3['Fare'].std())\n",
    "print(df3['Fare'].max())\n",
    "print(df3['Fare'].min())\n",
    "print(df3['Fare'].count())\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0daef0f-7cab-4b74-bf65-55762194bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACCESS PARTICULAR DATA BASED ON CONDITION\n",
    "\n",
    "df3[(df3['Sex'] == 'female') & (df3['Fare'] > 50.00)]\n",
    "df3[(df3['Sex'] == 'female') & (df7['Fare'] > 80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e55cd6-a308-4f90-8df1-c6f493385477",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict = {\"name\":['sahil','rohit','keshav'],\n",
    "        \"id\":[101,102,103],\n",
    "        \"city\":['Pune','banglore','mumbai']}\n",
    "df8 = pd.DataFrame(dict)\n",
    "print(df8['id'])\n",
    "df9 = df8.copy()\n",
    "df9['id'] = pd.Series([1000,220,4000])\n",
    "df9['name'] = pd.Series(['Ajay',\"Swara\",'Tanya'])\n",
    "df9['city'] = pd.Series(['PCMC','Bombay','Delhi'])\n",
    "df9.rename({'name':'n',\"id\":'i',\"city\":'c'},axis = 1 , inplace = True)\n",
    "\n",
    "display(df8[(df8['name'].str.contains('i')) & (df8['id']>100)])\n",
    "display(df9)\n",
    "display(df8[(df9['n'].str.contains('a')) | (df9['i']>3999)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f5fcf-0fa5-4d57-b433-fde76eb30aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GROUPBY \n",
    "\n",
    "obj = df3.groupby('Sex')\n",
    "for name , group in obj:\n",
    "    display(name)\n",
    "    display(group)\n",
    "obj.get_group('male')['Fare'].max()\n",
    "dir(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af291c20-d1d6-49f0-839d-5b622ff664f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obj.agg()\n",
    "#1st - url -- file path\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "df3 = pd.read_csv(url)\n",
    "obj2 = df3.groupby('Age')\n",
    "\"\"\"for name , group in obj2:\n",
    "    display(name)\n",
    "    display(group)\"\"\"\n",
    "\n",
    "obj2.get_group(24.0)\n",
    "agg_df1 = obj2.agg({'Fare':'max'})\n",
    "agg_df1\n",
    "agg_df2 = obj2.agg({'Fare':['sum','max','min']})\n",
    "agg_df2.loc[[24.0],:]\n",
    "agg_df3 = obj2.agg({'Fare':['sum','max','min'] , 'Survived':'sum'})\n",
    "agg_df3['Survived'].max()\n",
    "agg_df4 = obj2.agg({'Fare' : lambda x : x.max() - x.min()})\n",
    "agg_df4.max()\n",
    "agg_df5 = obj2.agg(Sum_Fare = ('Fare','sum'))\n",
    "agg_df5\n",
    "merged_df = obj2.agg({'Fare':'max'}).merge( obj2.agg({'Fare':'min'}) ,  on='Age' , suffixes=('_max','_min'))\n",
    "merged_df\n",
    "agg_df6 = df3.groupby('Sex').Fare.agg(['sum','min','max'])\n",
    "agg_df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ae10d-8068-409b-9c20-d1d0717c1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "employees = pd.DataFrame({\n",
    "    'emp_id': [101, 102, 103, 104],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'dept_id': [1, 2, 7, 3]\n",
    "})\n",
    "\n",
    "departments = pd.DataFrame({\n",
    "    'dept_id': [1, 4, 3],\n",
    "    'dept_name': ['HR', 'Engineering', 'Marketing']\n",
    "})\n",
    "\n",
    "pd.merge(employees,departments, on = 'dept_id', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627bebb-b787-4137-b143-3fbb5e7eb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANK\n",
    "\n",
    "data = {\n",
    "    \"id\": [1, 2, 3,4, 5, 6,7,8],\n",
    "    \"name\": [\"Alice1\", \"Bob1\", \"Charlie1\",\"Alice2\", \"Bob2\", \"Charlie2\",\"Bob3\", \"Charlie3\"],\n",
    "    \"sal\": [50000, 60000, 70000,80000,80900,60000,5573578,8348938],\n",
    "    \"city\": [\"New York\", \"Los Angeles\", \"Chicago\",\"New York\", \"New York\", \"Los Angeles\",\"Los Angeles\", \"Chicago\"],\n",
    "    \"yoexp\": [5, 8, 10,5, 8, 10,8, 10]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "display(df)\n",
    "df['rank'] = df.sort_values('sal' , ascending=False).groupby('city').cumcount()+1\n",
    "display(df)\n",
    "display(df.sort_values('sal' , ascending=False).groupby('city').get_group('New York'))\n",
    "display(df.sort_values('sal' , ascending=False).groupby('city').get_group('Chicago'))\n",
    "display(df.sort_values('sal' , ascending=False).groupby('city').get_group('Los Angeles'))\n",
    "data1 = data\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a dataset with duplicate values\n",
    "data = pd.DataFrame({\n",
    "    'Category': ['A', 'A', 'B', 'A', 'B', 'B', 'A', 'B', 'A', 'B'],\n",
    "    'Value': [10, 20, 30, 10, 50, 30, 20, 50, 10, 30]\n",
    "})\n",
    "display(data)\n",
    "\n",
    "# Applying cumcount()\n",
    "data['cumcount'] = data.groupby(['Category', 'Value']).cumcount()+1\n",
    "display(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f9c19-248d-40c7-abd1-19ed947bde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.concat([df,data],axis = 1, keys=['df1','df2']))\n",
    "display(pd.concat([df,data],axis = 0, keys=['df1','df2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb94440-f7af-4cc6-9ad8-f61d60f25ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = df.groupby('city')\n",
    "df_list = []\n",
    "for name , group in obj:\n",
    "    group = group.sort_values('sal',ascending=False)\n",
    "    group['rank'] = group['sal'].rank(method = 'dense' , ascending=False)\n",
    "    df_list.append(group)\n",
    "con = pd.concat(df_list, axis = 0, keys = [f'df_{df}' for df in range(0,len(df_list))])\n",
    "display(con)\n",
    "\n",
    "#df['rank'] = obj.get_group('New York').rank(method = 'dense')\n",
    "df['dense_rank'] = df['sal'].rank(method = 'dense')\n",
    "df\n",
    "\n",
    "obj2 = df.groupby('city')\n",
    "df_list3 = []\n",
    "for name , group in obj:\n",
    "    group['rank'] = group.sort_values('sal',ascending=False)['sal'].rank(method = 'dense' , ascending=False)\n",
    "    df_list3.append(group)\n",
    "con2 = pd.concat(df_list, axis = 0, keys = [f'df_{df}' for df in range(0,len(df_list3))])\n",
    "display(con2)\n",
    "\n",
    "#df['rank'] = obj.get_group('New York').rank(method = 'dense')\n",
    "df['dense_rank'] = df['sal'].rank(method = 'dense' )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c27a9-5418-44f9-9b13-fc5e04af2e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"id\": [1, 2, 3,4, 5, 6,7,8],\n",
    "    \"name\": [\"Alice1\", \"Bob1\", \"Charlie1\",\"Alice2\", \"Bob2\", \"Charlie2\",\"Bob3\", \"Charlie3\"],\n",
    "    \"sal\": [50000, 60000, 70000,80000,80900,60000,5573578,8348938],\n",
    "    \"city\": [\"New York\", \"Los Angeles\", \"Chicago\",\"New York\", \"New York\", \"Los Angeles\",\"Los Angeles\", \"Chicago\"],\n",
    "    \"yoexp\": [5, 8, 10,5, 8, 10,8, 10]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df = df.sort_values('sal')\n",
    "df['first'] = df['sal'].rank(method='first')\n",
    "df['max'] = df['sal'].rank(method='max')\n",
    "df['min'] = df['sal'].rank(method='min')\n",
    "df['avg'] = df['sal'].rank(method='average')\n",
    "df['dense'] = df['sal'].rank(method='dense')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88805e4c-9996-4379-a028-873a8884bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests , json , pprint\n",
    "\n",
    "response = requests.get(\"https://microsoftedge.github.io/Demos/json-dummy-data/64KB.json\")\n",
    "content = response.content\n",
    "#pprint.pprint(content)\n",
    "dict_js = json.loads(content)\n",
    "pprint.pprint(dict_js)\n",
    "df_js = pd.DataFrame(dict_js)\n",
    "df_js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22073a7-dbd0-4136-b926-19c8429f3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "json_data = '''\n",
    "[\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"salary\": 50000},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"salary\": 60000},\n",
    "    {\"id\": 3, \"name\": \"Charlie\", \"salary\": 70000}\n",
    "]\n",
    "'''\n",
    "\n",
    "df = pd.read_json('config.json' , orient= 'index')\n",
    "print(df)\n",
    "import pandas as pd\n",
    "\n",
    "data = {\"name\": \"Alice\", \"age\": 25, \"salary\": 50000}\n",
    "\n",
    "df = pd.DataFrame(data,index=[0])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a890f587-b3a3-4ba4-a86b-afb5bc91d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd.read_json('large-file.json')\n",
    "pd.DataFrame(df_long.actor.iloc[2] , index = [0])\n",
    "for col in range(df_long.shape[-1]):\n",
    "    for row in range(df_long.shape[0]):\n",
    "        try:\n",
    "            if type(df_long.iloc[row,col]) == type(dict):\n",
    "                df_long.iloc[row,col] = [pd.DataFrame(df_long.iloc[row,col] , index = [0])]\n",
    "        except:\n",
    "                try:\n",
    "                    df_long.iloc[row,col] = [[pd.DataFrame(df_long.iloc[row,col] , index = [0])]]\n",
    "                except:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e0fcd8-005f-479b-939f-c5f60789d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for col in range(2,df_long.shape[-1]):\n",
    "    for row in range(df_long.shape[0]):\n",
    "        try:\n",
    "                display(df_long.iloc[row,col][0])\n",
    "        except:\n",
    "                continue\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976901e3-e062-4bc7-b43d-7496010c029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html = pd.read_html('https://en.wikipedia.org/wiki/List_of_deaths_due_to_COVID-19')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca92d474-f51e-4344-9a57-483dddd8400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html[5].to_html('table3.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1038d6-6d42-4f48-ab88-f90fec152c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df_html[5]['Last updated'] = datetime.datetime.now()\n",
    "df_html[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70765b13-d09e-47ea-8e98-098bbc0b3de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_dict = {'id':[101,102,103],\n",
    "                'name' : ['A','B','C'],\n",
    "                  \"points\":[100,200,300]\n",
    "              }\n",
    "df34 = pd.DataFrame(encode_dict)\n",
    "df34\n",
    "                    \n",
    "\n",
    "def col_encode(val,p):\n",
    "    if val == 100:\n",
    "        return \"low\"\n",
    "    elif val == 200:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "    \n",
    "\n",
    "df34[\"code\"] = df34['points'].apply(lambda x: col_encode(x,'p'))\n",
    "\n",
    "def col_encode(val):\n",
    "    if val < 50:\n",
    "        return \"low\"\n",
    "    elif val > 50:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "df34[\"code\"] = df34['points'].map(col_encode) #map -- only one args\n",
    "\n",
    "df34\n",
    "df_html[5]['code'] = df_html[5].Age.map(col_encode)\n",
    "df_html[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df60266-05ec-44cb-a7a6-f1790469ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "\n",
    "with open('config.json') as fp:\n",
    "    dict = json.load(fp)\n",
    "connection_str = 'mysql+pymysql://{}:{}@{}:{}/{}'.format(dict[\"username\"],dict[\"password\"],dict[\"host\"],dict[\"port\"],dict[\"Database\"])\n",
    "conn = create_engine(connection_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb1ead-ae7f-4c85-9ec9-217a60db479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39968ed6-4e00-4aa1-9f77-bbc455ea4708",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('select * from exam',conn)\n",
    "pd.read_sql('exam',conn)\n",
    "pd.read_sql_table('exam',conn)\n",
    "pd.read_sql_query('select * from exam',conn)\n",
    "df_html[5].to_sql(name='corona23',\n",
    "                 if_exists='fail',\n",
    "                 con = conn\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f0dfc-ff97-4230-8bea-add511855a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_table('corona23',conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e50f4f-3377-4c22-949f-ab696580ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df40 = pd.read_csv(\"Amazon Sale Report.csv\" , low_memory=False)\n",
    "temp = df40['ship-city'].value_counts().to_frame()\n",
    "temp[temp['count'] >= 50]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
